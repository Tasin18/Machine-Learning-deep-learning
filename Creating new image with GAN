{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\npip install --upgrade tensorflow==2.11.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\npip install skillsnetwork","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport numpy as np\nimport tensorflow as tf\nimport keras\nprint(\"tensorflow version \",tf.__version__)\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input,Conv2DTranspose,BatchNormalization,ReLU,Conv2D,LeakyReLU\nfrom tensorflow.keras.models import Sequential\n\nfrom IPython import display\nimport skillsnetwork\nprint(skillsnetwork.__version__)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfrom os import listdir\nfrom pathlib import Path\n\nimport imghdr\nimport time\nfrom tqdm.auto import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:54:58.472386Z","iopub.execute_input":"2024-06-12T16:54:58.472948Z","iopub.status.idle":"2024-06-12T16:55:02.190713Z","shell.execute_reply.started":"2024-06-12T16:54:58.472916Z","shell.execute_reply":"2024-06-12T16:55:02.189740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This function will plot 5 images from an array","metadata":{}},{"cell_type":"code","source":"def plot_array(X,title=\"\"):\n    plt.rcParams['figure.figsize'] = (20,20)\n    \n    for i,x in enumerate(X[0:5]):\n        x = x.numpy()\n        max_ = x.max()\n        min_ = x.min()\n        xnew = np.uint(255*(x-min_)/(max_-min_))\n        plt.subplot(1,5,i+1)\n        plt.imshow(xnew)\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:07.213661Z","iopub.execute_input":"2024-06-12T16:55:07.214636Z","iopub.status.idle":"2024-06-12T16:55:07.220710Z","shell.execute_reply.started":"2024-06-12T16:55:07.214604Z","shell.execute_reply":"2024-06-12T16:55:07.219655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying Convolutional Neural Networks to GANS has led to improved results. They are called Deep Convolutional Generative Adversarial Networks (DCGANs). In this lab, we will build and train DCGANs using several approaches introduced in the original <a href=\"https://arxiv.org/pdf/1511.06434.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">DCGANs paper</a>. \n\n\nThe proposed approaches are summarized here:\n\n\n- Replace any pooling layers with **strided convolutions (discriminator)** and **fractional-strided\nconvolutions (generator)**.\n- Use **batchnorm** in both the generator and the discriminator.\n- **Remove fully connected hidden layers** for deeper architectures.\n- Use **ReLU** activation in generator for all layers except for the output, which uses **Tanh**.\n- Use **LeakyReLU** activation in the discriminator for all layers except for the output, which uses **Sigmoid**.\n- Use **Adam optimizer**.  \n","metadata":{}},{"cell_type":"code","source":"dataset_url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module5/L2/cartoon_data.tgz\"\nawait skillsnetwork.prepare(dataset_url, overwrite=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:10.044297Z","iopub.execute_input":"2024-06-12T16:55:10.045170Z","iopub.status.idle":"2024-06-12T16:55:42.098503Z","shell.execute_reply.started":"2024-06-12T16:55:10.045136Z","shell.execute_reply":"2024-06-12T16:55:42.097604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Anime Face or the Cartoon images are stored in the `cartoon_2000` folder in your current working directory. As a preprocessing step, we have removed any files that are not proper image formats (based on the file extensions) and any duplicate images.\n","metadata":{}},{"cell_type":"code","source":"img_height, img_width, batch_size=64,64,128","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:42.127829Z","iopub.execute_input":"2024-06-12T16:55:42.128078Z","iopub.status.idle":"2024-06-12T16:55:42.132402Z","shell.execute_reply.started":"2024-06-12T16:55:42.128055Z","shell.execute_reply":"2024-06-12T16:55:42.131419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we create a Keras <code>image_dataset_from_directory</code> object with a specified image directory and the parameters are defined as above.","metadata":{}},{"cell_type":"code","source":"train_ds = tf.keras.utils.image_dataset_from_directory(directory=\"/kaggle/working/cartoon_data\",\n                                                      image_size=(img_height,img_width),\n                                                      batch_size=batch_size,\n                                                      label_mode=None)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:42.134978Z","iopub.execute_input":"2024-06-12T16:55:42.135329Z","iopub.status.idle":"2024-06-12T16:55:45.941254Z","shell.execute_reply.started":"2024-06-12T16:55:42.135288Z","shell.execute_reply":"2024-06-12T16:55:45.939460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(OPTIONAL)** If you are running this notebook locally and you have multiple cores, then we can use the runtime to tune the value dynamically at runtime as follows:\n","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:45.942995Z","iopub.execute_input":"2024-06-12T16:55:45.943446Z","iopub.status.idle":"2024-06-12T16:55:45.954642Z","shell.execute_reply.started":"2024-06-12T16:55:45.943411Z","shell.execute_reply":"2024-06-12T16:55:45.953624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We apply the Lambda function on `train_ds` to normalize the pixel values of all the input images from $[0, 255]$ to $[-1, 1]$:\n","metadata":{}},{"cell_type":"code","source":"normalization_layer = layers.experimental.preprocessing.Rescaling(scale=1./127.5,offset=-1)\nnormalized_ds = train_ds.map(lambda x: normalization_layer(x))","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:45.955948Z","iopub.execute_input":"2024-06-12T16:55:45.956247Z","iopub.status.idle":"2024-06-12T16:55:46.016434Z","shell.execute_reply.started":"2024-06-12T16:55:45.956217Z","shell.execute_reply":"2024-06-12T16:55:46.015683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take one batch of images for displaying:\n","metadata":{}},{"cell_type":"code","source":"images = train_ds.take(1)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:46.017421Z","iopub.execute_input":"2024-06-12T16:55:46.017701Z","iopub.status.idle":"2024-06-12T16:55:46.022718Z","shell.execute_reply.started":"2024-06-12T16:55:46.017661Z","shell.execute_reply":"2024-06-12T16:55:46.021727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert the batch dimension to the indexes in a list:\n","metadata":{}},{"cell_type":"code","source":"X = [x for x in images]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:46.023882Z","iopub.execute_input":"2024-06-12T16:55:46.024206Z","iopub.status.idle":"2024-06-12T16:55:52.789881Z","shell.execute_reply.started":"2024-06-12T16:55:46.024176Z","shell.execute_reply":"2024-06-12T16:55:52.788648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_array(X[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:52.792903Z","iopub.execute_input":"2024-06-12T16:55:52.793201Z","iopub.status.idle":"2024-06-12T16:55:53.205264Z","shell.execute_reply.started":"2024-06-12T16:55:52.793175Z","shell.execute_reply":"2024-06-12T16:55:53.204007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Generator is comprised of several layers of transposed convolution, the opposite of convolution operations.\n\n- Each Conv2DTranspose layer (except the final layer) is followed by a Batch Normalization layer and a **Relu activation**; for more implementation details, check out <a href=\"https://arxiv.org/pdf/1511.06434.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">[1]</a>. \n- The final transpose convolution layer has three output channels since the output needs to be a color image. We use the **Tanh activation** in the final layer. \n\nSee the illustration of the architecture from <a href=\"https://arxiv.org/pdf/1511.06434.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">[1]</a> below.\n\n<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/generator.png\" alt=\"generator image\" width=\"80%\"></center>\n\nWe build the Generator network by using the parameter values from <a href=\"https://learnopencv.com/deep-convolutional-gan-in-pytorch-and-tensorflow/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" >[2]<a>.\n","metadata":{}},{"cell_type":"code","source":"def make_generator():\n    model = Sequential()\n    #input is a latent vector of 100 dimensions\n    model.add(Input(shape=(1,1,100),name='input_layer'))\n    \n    #block 1 dimensionality of the output space 64*8\n    model.add(Conv2DTranspose(64*8,kernel_size=4,strides=4,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02),use_bias=False,name='conv_transpose_1'))\n    model.add(BatchNormalization(momentum=0.1,epsilon=0.8,center=1,scale=0.02,name='bn_1'))\n    model.add(ReLU(name='relu_1'))\n    \n    #block 2 input is 4*4*(64*8)\n    model.add(Conv2DTranspose(64*4,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02),use_bias=False,name='conv_transpose_2'))\n    model.add(BatchNormalization(momentum=0.1,epsilon=0.8,center=1,scale=0.02,name='bn_2'))\n    model.add(ReLU(name='relu_2'))\n    \n    #block 3 input is 8*8(64*4)\n    model.add(Conv2DTranspose(64*2,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02),use_bias=False,name='conv_transpose_3'))\n    model.add(BatchNormalization(momentum=0.1,epsilon=0.8,center=1,scale=0.02,name='bn_3'))\n    model.add(ReLU(name='relu_3'))\n    \n    #block 4 input is 16*16(64*2)\n    model.add(Conv2DTranspose(64*1,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02),use_bias=False,name='conv_transpose_4'))\n    model.add(BatchNormalization(momentum=0.1,epsilon=0.8,center=1,scale=0.02,name='bn_4'))\n    model.add(ReLU(name='relu_4'))\n    \n    model.add(Conv2DTranspose(3,4,2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.02),use_bias=False,\n                             activation='tanh',name='conv_transpose_5'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.206705Z","iopub.execute_input":"2024-06-12T16:55:53.207362Z","iopub.status.idle":"2024-06-12T16:55:53.232048Z","shell.execute_reply.started":"2024-06-12T16:55:53.207313Z","shell.execute_reply":"2024-06-12T16:55:53.230251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen = make_generator()\ngen.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.272692Z","iopub.execute_input":"2024-06-12T16:55:53.272915Z","iopub.status.idle":"2024-06-12T16:55:53.494352Z","shell.execute_reply.started":"2024-06-12T16:55:53.272894Z","shell.execute_reply":"2024-06-12T16:55:53.493507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Discriminator has five convolution layers. \n\n- All but the first and final Conv2D layers have Batch Normalization, since directly applying batchnorm to all layers could result in sample oscillation and model instability; \n- The first four Conv2D layers use the **Leaky-Relu activation** with a slope of 0.2. \n- Lastly, instead of a fully connected layer, the  output layer has a convolution layer with a **Sigmoid activation** function.\n","metadata":{}},{"cell_type":"code","source":"def make_discriminator():\n    model = Sequential()\n    #block 1 input is 64*64*3\n    model.add(Input(shape=(64,64,3),name='input_layer'))\n    model.add(Conv2D(64,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0,stddev=0.02),use_bias=False,name='conv_1'))\n    model.add(LeakyReLU(0.2,name='leaky_1'))\n    #block 2 input 32*32*64\n    model.add(Conv2D(64*2,kernel_size=4,strides=2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0,stddev=0.02),use_bias=False,name='conv_2'))\n    model.add(BatchNormalization(momentum=0.1,epsilon=0.8,center=1.0,scale=0.02,name='bn_1'))\n    model.add(LeakyReLU(0.2,name='leaky_2'))\n    #block 3 input 16*16*128\n    model.add(Conv2D(64*4,4,2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0,stddev=0.02),use_bias=False,name='conv_3'))\n    model.add(BatchNormalization(momentum=0.1,epsilon=0.8,center=1,scale=0.02,name='bn_2'))\n    model.add(LeakyReLU(0.2,name='leaky_3'))\n    #block 4 input 8*8*256\n    model.add(Conv2D(64*8,4,2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0,stddev=0.02),use_bias=False,name='conv_4'))\n    model.add(BatchNormalization(momentum=0.1,epsilon=0.8,center=1,scale=0.02,name='bn_3'))\n    model.add(LeakyReLU(0.2,name='leaky_4'))\n    #block 5\n    model.add(Conv2D(1,4,2,padding='same',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0,stddev=0.02),use_bias=False,\n                    activation='sigmoid',name='conv_5'))\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.496025Z","iopub.execute_input":"2024-06-12T16:55:53.496628Z","iopub.status.idle":"2024-06-12T16:55:53.511753Z","shell.execute_reply.started":"2024-06-12T16:55:53.496593Z","shell.execute_reply":"2024-06-12T16:55:53.510838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disc = make_discriminator()\ndisc.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.516217Z","iopub.execute_input":"2024-06-12T16:55:53.516651Z","iopub.status.idle":"2024-06-12T16:55:53.659068Z","shell.execute_reply.started":"2024-06-12T16:55:53.516618Z","shell.execute_reply":"2024-06-12T16:55:53.658214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Loss Functions","metadata":{}},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.660223Z","iopub.execute_input":"2024-06-12T16:55:53.660507Z","iopub.status.idle":"2024-06-12T16:55:53.665722Z","shell.execute_reply.started":"2024-06-12T16:55:53.660480Z","shell.execute_reply":"2024-06-12T16:55:53.663885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(xhat):\n    return cross_entropy(tf.ones_like(xhat),xhat)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.666882Z","iopub.execute_input":"2024-06-12T16:55:53.667144Z","iopub.status.idle":"2024-06-12T16:55:53.675518Z","shell.execute_reply.started":"2024-06-12T16:55:53.667121Z","shell.execute_reply":"2024-06-12T16:55:53.674707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(X,xhat):\n    real_loss = cross_entropy(tf.ones_like(X),X)\n    fake_loss = cross_entropy(tf.zeros_like(xhat),xhat)\n    total_loss = 0.5*(real_loss + fake_loss)\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.676742Z","iopub.execute_input":"2024-06-12T16:55:53.677026Z","iopub.status.idle":"2024-06-12T16:55:53.685627Z","shell.execute_reply.started":"2024-06-12T16:55:53.677002Z","shell.execute_reply":"2024-06-12T16:55:53.684718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Optimizers \n \nWe create two Adam optimizers for the discriminator and the generator, respectively. We pass the following arguments to the optimizers:\n\n- learning rate of 0.0002.\n- beta coefficients $\\beta_1 = 0.5$ and $\\beta_2 = 0.999$, which are responsible for computing the running averages of the gradients during backpropagation.\n","metadata":{}},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002,beta_1=0.5,beta_2=0.999)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002,beta_1=0.5,beta_2=0.999)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.686729Z","iopub.execute_input":"2024-06-12T16:55:53.686989Z","iopub.status.idle":"2024-06-12T16:55:53.700442Z","shell.execute_reply.started":"2024-06-12T16:55:53.686967Z","shell.execute_reply":"2024-06-12T16:55:53.699675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Train Step Function\n\nAs this lab is more computationally intensive than the last lab, we convert the training step into a function and then use the  @tf.function decorator, which allows the function to be \"compiled\" into a **callable TensorFlow graph**. This will speed up the training; for more information, read <a href=\"https://www.tensorflow.org/guide/function?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\">here </a> \n","metadata":{}},{"cell_type":"code","source":"@tf.function\n\ndef train_step(X):\n    #random samples it was found if you increase the  stander deviation, you get better results \n    z = tf.random.normal([BATCH_SIZE,1,1,latent_dim])\n    # needed to compute the gradients for a list of variables.\n    with tf.GradientTape() as gen_tape,tf.GradientTape() as disc_tape:\n        #generate sample\n        xhat = generator(z,training=True)\n        #the output of the discriminator for real data\n        real_output=discriminator(X,training=True)\n        #the output of the discriminator for fake data\n        fake_output=discriminator(xhat,training=True)\n        \n        #loss for each\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output,fake_output)\n        \n        # Compute the gradients for gen_loss and generator\n        gradients_of_generator = gen_tape.gradient(gen_loss,generator.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n        \n        # Ask the optimizer to apply the processed gradients\n        generator_optimizer.apply_gradients(zip(gradients_of_generator,generator.trainable_variables))\n        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,discriminator.trainable_variables))\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:53.701560Z","iopub.execute_input":"2024-06-12T16:55:53.701840Z","iopub.status.idle":"2024-06-12T16:55:53.710426Z","shell.execute_reply.started":"2024-06-12T16:55:53.701818Z","shell.execute_reply":"2024-06-12T16:55:53.709612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Don't be intimidated by the code above, here is a summary of what a train step accomplishes:\n\n- First, we sample `z`, a batch of noise vectors from a normal distribution ($\\mu = 1, \\sigma = 1$) and feed it to the Generator.\n- The Generator produces generated or \"fake\" images `xhat`.\n- We feed real images `X` and fake images `xhat` to the Discriminator and obtain `real_output` and `fake_output` respectively as the scores.\n- We calculate Generator loss `gen_loss` using the `fake_output` from Discriminator since we want the fake images to fool the Discriminator as much as possible.\n- We calculate Discriminator loss `disc_loss` using both the `real_output` and `fake_output` since we want the Discriminator to distinguish the two as much as possible.\n- We calculate `gradients_of_generator` and  `gradients_of_discriminator` based on the losses obtained.\n- Finally, we update the Generator and Discriminator by letting their respective optimizers apply the processed gradients on the trainable model parameters.\n","metadata":{}},{"cell_type":"code","source":"generator = make_generator()\nBATCH_SIZE=128\nlatent_dim=100\n\nnoise = tf.random.normal([BATCH_SIZE,1,1,latent_dim])\nxhat = generator(noise,training=False)\nplot_array(xhat)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:55:54.337819Z","iopub.execute_input":"2024-06-12T16:55:54.338777Z","iopub.status.idle":"2024-06-12T16:55:55.560446Z","shell.execute_reply.started":"2024-06-12T16:55:54.338736Z","shell.execute_reply":"2024-06-12T16:55:55.559202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=6\n\ndiscriminator=make_discriminator()\n\ngenerator= make_generator()\n\n\nfor epoch in range(epochs):\n    \n    #data for the true distribution of your real data samples training ste\n    start = time.time()\n    i=0\n    for X in tqdm(normalized_ds, desc=f\"epoch {epoch+1}\", total=len(normalized_ds)):\n        \n        i+=1\n        if i%1000:\n            print(\"epoch {}, iteration {}\".format(epoch+1, i))\n            \n        train_step(X)\n    \n\n    noise = tf.random.normal([BATCH_SIZE, 1, 1, latent_dim])\n    Xhat=generator(noise,training=False)\n    X=[x for x in normalized_ds]\n    print(\"orignal images\")\n    plot_array(X[0])\n    print(\"generated images\")\n    plot_array(Xhat)\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading Pre-trained model (150 epochs)\nAs you saw, training a GAN with only one epoch takes quite a long time. If we want to evaluate the performance of a fully trained and optimized GAN, we would need to increase the number of epochs. Thus, to help you avoid extremely long training time in this lab, we will just download the pre-trained Generator network parameters and then use Kera `load_model` function to obtain a pre-trained Generator, which we will use to generate images directly.\n","metadata":{}},{"cell_type":"code","source":"generator_url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/generator.tar.gz\"\nawait skillsnetwork.prepare(generator_url, overwrite=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T17:02:36.753739Z","iopub.execute_input":"2024-06-12T17:02:36.754127Z","iopub.status.idle":"2024-06-12T17:02:38.221733Z","shell.execute_reply.started":"2024-06-12T17:02:36.754094Z","shell.execute_reply":"2024-06-12T17:02:38.220718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nfull_generator = load_model('/kaggle/working/generator')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T17:04:04.005832Z","iopub.execute_input":"2024-06-12T17:04:04.006220Z","iopub.status.idle":"2024-06-12T17:04:04.722091Z","shell.execute_reply.started":"2024-06-12T17:04:04.006186Z","shell.execute_reply":"2024-06-12T17:04:04.721316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_dim=100\n\nnoise = tf.random.normal([200,1,1,latent_dim])\nxhat = full_generator(noise,training=False)\nplot_array(xhat)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T17:05:48.527281Z","iopub.execute_input":"2024-06-12T17:05:48.527648Z","iopub.status.idle":"2024-06-12T17:05:50.089200Z","shell.execute_reply.started":"2024-06-12T17:05:48.527618Z","shell.execute_reply":"2024-06-12T17:05:50.087736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore Latent Variables \nValues of $\\mathbf{z}$ that are relatively close together will produce similar images. For example, we can assigns elements of $\\mathbf{z}$ close values such as $[1,0.8,..,0.4]$. \n","metadata":{}},{"cell_type":"code","source":"for c in [1,0.8,0.6,0.5]:\n    xhat = full_generator(c*tf.ones([1,1,1,latent_dim]),training=False)\n    plot_array(xhat)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T17:10:54.989045Z","iopub.execute_input":"2024-06-12T17:10:54.989813Z","iopub.status.idle":"2024-06-12T17:10:55.394545Z","shell.execute_reply.started":"2024-06-12T17:10:54.989782Z","shell.execute_reply":"2024-06-12T17:10:55.393231Z"},"trusted":true},"execution_count":null,"outputs":[]}]}