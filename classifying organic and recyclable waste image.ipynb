{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2>Table of Contents</h2>\n\nIn this lab, we train a Convolutional Neural Network with Regular data and Augmented data. The purpose of this lab is to show that the Augmented data improves generalization performance.\n\nThink of a scenario where a drone has to take a picture of an object. The drone is moving and the object can also possibly be moving. When an image is taken we arent always going to get perfect images. The subject may not be perfectly centered in the image or the subject may be rotated in the image. In this case, a model trained on perfectly centered or rotated images won't perform well. This is why we train a model on rotated data so it can perform well on imperfect images.\n\nIn this assignment, we will use a dataset of digit images. We will have two models one trained on non rotated digits and one trained on rotated images and then we will test the models on a rotated testing dataset which will be more realistic and robust in terms of our scenario above.\n\n<ul>\n<li><a href=\"#Makeup_Data\">Get Some Data</a></li>\n<li><a href=\"#CNN\">Convolutional Neural Network</a></li>\n<li><a href=\"#R_training_data\">Rotated Training Data</a></li>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:23:30.406309Z","iopub.execute_input":"2024-06-21T17:23:30.407361Z","iopub.status.idle":"2024-06-21T17:23:42.625013Z","shell.execute_reply.started":"2024-06-21T17:23:30.407327Z","shell.execute_reply":"2024-06-21T17:23:42.623983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download a Pretrained Model because training takes a long time\n!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/meet_up/12.02.2020/normal.pt","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:23:42.627177Z","iopub.execute_input":"2024-06-21T17:23:42.627509Z","iopub.status.idle":"2024-06-21T17:23:43.791507Z","shell.execute_reply.started":"2024-06-21T17:23:42.627480Z","shell.execute_reply":"2024-06-21T17:23:43.790580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download a Pretrained Model Trained on Augmented Data because training takes a long time\n!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/meet_up/12.02.2020/rotated_data.pt","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:23:43.793129Z","iopub.execute_input":"2024-06-21T17:23:43.794019Z","iopub.status.idle":"2024-06-21T17:23:45.029573Z","shell.execute_reply.started":"2024-06-21T17:23:43.793980Z","shell.execute_reply":"2024-06-21T17:23:45.028565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Library to Show Images\n!pip install Pillow","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:23:45.032055Z","iopub.execute_input":"2024-06-21T17:23:45.032369Z","iopub.status.idle":"2024-06-21T17:23:57.121633Z","shell.execute_reply.started":"2024-06-21T17:23:45.032340Z","shell.execute_reply":"2024-06-21T17:23:57.120480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Restart the kernel***\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:23:57.123224Z","iopub.execute_input":"2024-06-21T17:23:57.124135Z","iopub.status.idle":"2024-06-21T17:23:57.129695Z","shell.execute_reply.started":"2024-06-21T17:23:57.124095Z","shell.execute_reply":"2024-06-21T17:23:57.128662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot Cost and Accuracy vs Epoch Graph\n","metadata":{}},{"cell_type":"code","source":"def plot_cost_accuracy(checkpoint):\n    fig, ax1 = plt.subplots()\n    color='tab:red'\n    ax1.plot(checkpoint['cost'],color=color)\n    ax1.set_xlabel('epoch',color=color)\n    ax1.set_ylabel('cost',color=color)\n    ax1.tick_params(axis='y',color=color)\n    \n    ax2=ax1.twinx()\n    color='tab:blue'\n    ax2.plot(checkpoint['accuracy'],color=color)\n    ax2.set_xlabel('epoch',color=color)\n    ax1.set_ylabel('accuracy',color=color)\n    ax1.tick_params(axis='y',color=color)\n    fig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:23:57.130924Z","iopub.execute_input":"2024-06-21T17:23:57.131205Z","iopub.status.idle":"2024-06-21T17:23:57.140527Z","shell.execute_reply.started":"2024-06-21T17:23:57.131182Z","shell.execute_reply":"2024-06-21T17:23:57.139628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the function <code>show_data</code> to plot out data samples as images.\n","metadata":{}},{"cell_type":"code","source":"def show_data(data_sample):\n    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE,IMAGE_SIZE),cmap='gray')\n    plt.title('y='+str(data_sample[1]))\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:23:57.141880Z","iopub.execute_input":"2024-06-21T17:23:57.142245Z","iopub.status.idle":"2024-06-21T17:23:57.149778Z","shell.execute_reply.started":"2024-06-21T17:23:57.142215Z","shell.execute_reply":"2024-06-21T17:23:57.148919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot first 5 misclassified samples \n","metadata":{}},{"cell_type":"code","source":"def plot_mis_classified(model,dataset):\n    count=0\n    for x,y in torch.utils.data.DataLoader(dataset=dataset,batch_size=1):\n        z = model(x)\n        _,yhat = torch.max(z,1)\n        if yhat != y:\n            show_data((x,y))\n            plt.show()\n            count +=1\n        if count >= 5:\n            break","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:23:57.150984Z","iopub.execute_input":"2024-06-21T17:23:57.151808Z","iopub.status.idle":"2024-06-21T17:23:57.161102Z","shell.execute_reply.started":"2024-06-21T17:23:57.151783Z","shell.execute_reply":"2024-06-21T17:23:57.160315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"Makeup_Data\">Load Data</h2> \n","metadata":{}},{"cell_type":"markdown","source":"We create a transform object  <code>compose</code> one will resize the image and convert it to a tensor, the second will also rotate the image Randomly rotate the image.\n","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE=16\n\ncompose_rotate = transforms.Compose([transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),transforms.RandomAffine(45),transforms.ToTensor()])\n\ncompose = transforms.Compose([transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),transforms.ToTensor()])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:24:27.918637Z","iopub.execute_input":"2024-06-21T17:24:27.919533Z","iopub.status.idle":"2024-06-21T17:24:27.925312Z","shell.execute_reply.started":"2024-06-21T17:24:27.919497Z","shell.execute_reply":"2024-06-21T17:24:27.924218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_rotate = dsets.MNIST(root='/kaggle/working/',train=True,download=True,transform=compose_rotate)\ntrain_dataset=dsets.MNIST(root='/kaggle/working/',train=True,download=True,transform=compose)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:27:13.319705Z","iopub.execute_input":"2024-06-21T17:27:13.320491Z","iopub.status.idle":"2024-06-21T17:27:17.211740Z","shell.execute_reply.started":"2024-06-21T17:27:13.320451Z","shell.execute_reply":"2024-06-21T17:27:17.210792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_dataset = dsets.MNIST(root='/kaggle/working/',train=False,download=True,transform=compose_rotate)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:29:12.571251Z","iopub.execute_input":"2024-06-21T17:29:12.572552Z","iopub.status.idle":"2024-06-21T17:29:12.588551Z","shell.execute_reply.started":"2024-06-21T17:29:12.572508Z","shell.execute_reply":"2024-06-21T17:29:12.587480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_data(train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:30:00.289595Z","iopub.execute_input":"2024-06-21T17:30:00.290298Z","iopub.status.idle":"2024-06-21T17:30:00.714324Z","shell.execute_reply.started":"2024-06-21T17:30:00.290266Z","shell.execute_reply":"2024-06-21T17:30:00.713331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0][1]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:30:26.409813Z","iopub.execute_input":"2024-06-21T17:30:26.410748Z","iopub.status.idle":"2024-06-21T17:30:26.417695Z","shell.execute_reply.started":"2024-06-21T17:30:26.410713Z","shell.execute_reply":"2024-06-21T17:30:26.416743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_data(train_dataset_rotate[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:30:48.810701Z","iopub.execute_input":"2024-06-21T17:30:48.811081Z","iopub.status.idle":"2024-06-21T17:30:49.127916Z","shell.execute_reply.started":"2024-06-21T17:30:48.811054Z","shell.execute_reply":"2024-06-21T17:30:49.126921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"CNN\">Build a Convolutional Neural Network Class</h2>\nBuild a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layers.\n","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    #constructor\n    def __init__(self,out_1=16,out_2=32):\n        super(CNN,self).__init__()\n        # The reason we start with 1 channel is because we have a single black and white image\n        # Channel Width after this layer is 16\n        self.cnn1 = nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=5,padding=2)\n        # Channel Width after this layer is 8\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n        \n        self.cnn2 = nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=5,stride=1,padding=2)\n        # Channel Width after this layer is 4\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        # In total we have out_2 (32) channels which are each 4 * 4 in size based on the width calculation above. Channels are squares.\n        # The output is a value for each class\n        self.fc1 = nn.Linear(out_2*4*4,10)\n    #prediction\n    def forward(self,x):\n        x = self.cnn1(x)\n        x = torch.relu(x)\n        x=self.maxpool1(x)\n        x=self.cnn2(x)\n        x=torch.relu(x)\n        x=self.maxpool2(x)\n        x=x.view(x.size(0),-1)\n        x=self.fc1(x)\n        return x\n    def activations(self,x):\n        z1 = self.cnn1(x)\n        a1 = torch.relu(z1)\n        out=self.maxpool1(a2)\n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out1=self.maxpool2(z2)\n        return z1,a1,z2,a2,out1,out","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:48:28.046639Z","iopub.execute_input":"2024-06-21T17:48:28.047568Z","iopub.status.idle":"2024-06-21T17:48:28.057357Z","shell.execute_reply.started":"2024-06-21T17:48:28.047532Z","shell.execute_reply":"2024-06-21T17:48:28.056462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"regular_data\">Regular Data</h2> \nDefine the Convolutional Neural Network Classifier, Criterion function, Optimizer, and Train the Model\n","metadata":{}},{"cell_type":"code","source":"# Create the model object to be trained on regular data using CNN class\nmodel = CNN(out_1=16,out_2=32)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:48:32.083707Z","iopub.execute_input":"2024-06-21T17:48:32.084108Z","iopub.status.idle":"2024-06-21T17:48:32.090521Z","shell.execute_reply.started":"2024-06-21T17:48:32.084073Z","shell.execute_reply":"2024-06-21T17:48:32.089592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=5000)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T17:51:48.772074Z","iopub.execute_input":"2024-06-21T17:51:48.773062Z","iopub.status.idle":"2024-06-21T17:51:48.779746Z","shell.execute_reply.started":"2024-06-21T17:51:48.773027Z","shell.execute_reply":"2024-06-21T17:51:48.778793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n#location to save data\nfile_normal = os.path.join(os.getcwd(),'normal.pt')\n# All the data we are saving\ncheckpoint = {\n    'epoch':None,\n    'model_state_dict':None,\n    'optimizer_state_dict':None,\n    'loss':None,\n    'cost':[],\n    'accuracy':[]\n}\n\nn_epochs=5\n# Size of the testing dataset\nN_test = len(validation_dataset)\n# Training for the number of epochs we want\nfor epoch in tqdm(range(n_epochs)):\n    cost=0\n    for x,y in train_loader:\n        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n        optimizer.zero_grad()\n        # Makes a prediction on the image\n        z=model(x)\n        # Calculate the loss between the prediction and actual class\n        loss=criterion(z,y)\n        # Calculates the gradient value with respect to each weight and bias\n        loss.backward()\n        # Updates the weight and bias according to calculated gradient value\n        optimizer.step()\n        \n        # Saves the number of epochs we trained for  \n        checkpoint['epochs'] = n_epochs\n        # Saves the models parameters\n        checkpoint['model_state_dict'] = model.state_dict()\n        # Saves the optimizers paramters\n        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n        # Saves the loss for the last batch so ultimately this will be the loss for the last batch of the last epoch\n        checkpoint['loss'] = loss\n        # Accumulates the loss\n        cost += loss.item()\n    # Counter for the correct number of predictions        \n    correct = 0\n    # For each batch in the validation dataset\n    for x_test, y_test in validation_loader:\n        # Make a prediction\n        z = model(x_test)\n        # Get the class that has the maximum value\n        _, yhat = torch.max(z.data, 1)\n        # Counts the number of correct predictions made\n        correct += (yhat == y_test).sum().item()\n \n    accuracy = correct / N_test\n    print(accuracy)\n    # Appends the cost of the epoch to a list\n    checkpoint['cost'].append(cost) \n    # Appends the accuracy of the epoch to a list\n    checkpoint['accuracy'].append(accuracy)\n    # Saves the data in checkpoint to the file location\n    torch.save(checkpoint, file_normal) ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:05:40.231649Z","iopub.execute_input":"2024-06-21T18:05:40.232124Z","iopub.status.idle":"2024-06-21T18:07:02.415929Z","shell.execute_reply.started":"2024-06-21T18:05:40.232090Z","shell.execute_reply":"2024-06-21T18:07:02.414847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 id=\"Result\">Analyze Results</h3> \n","metadata":{}},{"cell_type":"code","source":"checkpoint_normal = torch.load(os.path.join(os.getcwd(),'normal.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:07:48.398523Z","iopub.execute_input":"2024-06-21T18:07:48.399510Z","iopub.status.idle":"2024-06-21T18:07:48.408673Z","shell.execute_reply.started":"2024-06-21T18:07:48.399465Z","shell.execute_reply":"2024-06-21T18:07:48.407456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_cost_accuracy(checkpoint_normal)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:08:14.712487Z","iopub.execute_input":"2024-06-21T18:08:14.713443Z","iopub.status.idle":"2024-06-21T18:08:15.167380Z","shell.execute_reply.started":"2024-06-21T18:08:14.713397Z","shell.execute_reply":"2024-06-21T18:08:15.166442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the model parameters we saved we load them into a model to recreate the trained model\nmodel.load_state_dict(checkpoint_normal['model_state_dict'])\n# Setting the model to evaluation mode\nmodel.eval()\n# Using the helper function plot the first five misclassified samples\nplot_mis_classified(model,validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:08:56.862161Z","iopub.execute_input":"2024-06-21T18:08:56.863248Z","iopub.status.idle":"2024-06-21T18:08:58.257143Z","shell.execute_reply.started":"2024-06-21T18:08:56.863210Z","shell.execute_reply":"2024-06-21T18:08:58.256192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"R_training_data\">Rotated Training Data</h2> \n","metadata":{}},{"cell_type":"code","source":"# Create the model object using CNN class\nmodel_r = CNN(out_1=16, out_2=32)\n# We create a criterion which will measure loss\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\n# Create an optimizer that updates model parameters using the learning rate and gradient\noptimizer = torch.optim.SGD(model_r.parameters(), lr = learning_rate)\n# Create a Data Loader for the rotated training data with a batch size of 100 \ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset_rotate, batch_size=100)\n# Create a Data Loader for the rotated validation data with a batch size of 5000 \nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:10:08.375862Z","iopub.execute_input":"2024-06-21T18:10:08.376650Z","iopub.status.idle":"2024-06-21T18:10:08.384847Z","shell.execute_reply.started":"2024-06-21T18:10:08.376616Z","shell.execute_reply":"2024-06-21T18:10:08.384018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Location to save data\nfile_rotated = os.path.join(os.getcwd(), 'rotated_data.pt')\n\n# All the data we are saving\ncheckpoint={\n            # Saving the number of epochs the models was trained for\n            'epoch': None,\n            # Saving the models parameters which will allow us to recreate the trained model\n            'model_state_dict': None,\n            # Saving the optimizers parameters\n            'optimizer_state_dict': None,\n            # Saving the loss on the training dataset for the last batch of the last epoch\n            'loss': None,\n            # Saving the cost on the training dataset for each epoch\n            'cost': [],\n            # Saving the accuracy for the testing dataset for each epoch\n            'accuracy': []}\n            \n# Number of epochs to train model            \nn_epochs = 10\n\n# Size of the testing dataset\nN_test = len(validation_dataset)\n\n# Training for the number of epochs we want\nfor epoch in tqdm(range(n_epochs)):\n    # Variable to keep track of cost for each epoch\n    cost = 0\n    # For each batch in the training dataset\n    for x, y in train_loader:\n        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n        optimizer.zero_grad()\n        # Makes a prediction on the image\n        z = model_r(x)\n        # Calculate the loss between the prediction and actual class\n        loss = criterion(z, y)\n        # Calculates the gradient value with respect to each weight and bias\n        loss.backward()\n        # Updates the weight and bias according to calculated gradient value\n        optimizer.step()\n      \n        # Saves the number of epochs we trained for  \n        checkpoint['epochs'] = n_epochs\n        # Saves the models parameters\n        checkpoint['model_state_dict'] = model.state_dict()\n        # Saves the optimizers paramters\n        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n        # Saves the loss for the last batch so ultimately this will be the loss for the last batch of the last epoch\n        checkpoint['loss'] = loss\n        # Accumulates the loss\n        cost+=loss.item()\n        \n     \n    # Counter for the correct number of predictions        \n    correct = 0\n        \n    # For each batch in the validation dataset\n    for x_test, y_test in validation_loader:\n        # Make a prediction\n        z = model_r(x_test)\n        # Get the class that has the maximum value\n        _, yhat = torch.max(z.data, 1)\n        # Counts the number of correct predictions made\n        correct += (yhat == y_test).sum().item()\n \n    accuracy = correct / N_test\n    print(accuracy)\n    # Appends the cost of the epoch to a list\n    checkpoint['cost'].append(cost) \n    # Appends the accuracy of the epoch to a list\n    checkpoint['accuracy'].append(accuracy)\n    # Saves the data in checkpoint to the file location\n    torch.save(checkpoint, file_rotated) ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:11:29.904375Z","iopub.execute_input":"2024-06-21T18:11:29.904824Z","iopub.status.idle":"2024-06-21T18:15:06.062002Z","shell.execute_reply.started":"2024-06-21T18:11:29.904785Z","shell.execute_reply":"2024-06-21T18:15:06.061029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_rotated= torch.load(os.path.join(os.getcwd(),'rotated_data.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:16:51.293725Z","iopub.execute_input":"2024-06-21T18:16:51.294499Z","iopub.status.idle":"2024-06-21T18:16:51.301224Z","shell.execute_reply.started":"2024-06-21T18:16:51.294464Z","shell.execute_reply":"2024-06-21T18:16:51.300265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_cost_accuracy(checkpoint_rotated)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:17:05.448553Z","iopub.execute_input":"2024-06-21T18:17:05.449263Z","iopub.status.idle":"2024-06-21T18:17:05.871625Z","shell.execute_reply.started":"2024-06-21T18:17:05.449232Z","shell.execute_reply":"2024-06-21T18:17:05.870672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the model parameters we saved we load them into a model to recreate the trained model\nmodel_r.load_state_dict(checkpoint_rotated['model_state_dict'])\n# Setting the model to evaluation mode\nmodel.eval()\n# Using the helper function plot the first five misclassified samples\nplot_mis_classified(model_r,validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:17:21.779373Z","iopub.execute_input":"2024-06-21T18:17:21.780203Z","iopub.status.idle":"2024-06-21T18:17:23.165431Z","shell.execute_reply.started":"2024-06-21T18:17:21.780171Z","shell.execute_reply":"2024-06-21T18:17:23.164482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}